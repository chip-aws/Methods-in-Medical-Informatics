{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Doublet Hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anydbm, string, re\n",
    "doubhash = anydbm.open(‘doub’, ‘n’)\n",
    "literalhash = anydbm.open(‘literal’, ‘n’)\n",
    "in_file = open(‘c:\\\\ftp\\\\neocl.xml’, “r”)\n",
    "singular = re.compile(‘omas’)\n",
    "england = re.compile(‘tumou?rs?’)\n",
    "phrase = “”\n",
    "for line in in_file:\n",
    "    neoplasm_match = re.search(r’\\”\\> ?(.+) ?\\<’, line)\n",
    "    if neoplasm_match:\n",
    "        phrase = neoplasm_match.group(1)\n",
    "        phrase = singular.sub(“oma”,phrase)\n",
    "        phrase = england.sub(“tumor”,phrase)\n",
    "        literalhash[phrase] = “”\n",
    "    hoparray = phrase.split()\n",
    "    hoparray.append(“ “)\n",
    "    for i in range(len(hoparray)-1):\n",
    "        doublet = hoparray[i] + “ “ + hoparray[i + 1]\n",
    "        if doubhash.has_key(doublet):\n",
    "            continue\n",
    "        doubhash_match = re.search(r’[a-z]+ [a-z]+’, doublet)\n",
    "        if doubhash_match:\n",
    "            doubhash[doublet] = “”\n",
    "doubhash.close()\n",
    "literalhash.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scanning the Literature for Candidate Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anydbm, string, re\n",
    "doubhash = anydbm.open(‘doub’)\n",
    "literalhash = anydbm.open(‘literal’)\n",
    "newhash = {}\n",
    "in_file = open(‘c:\\\\big\\\\cancer_gene_titles.txt’, ‘r’)\n",
    "line = “ “\n",
    "count = 0\n",
    "singular = re.compile(‘omas’)\n",
    "england = re.compile(‘tumou?rs?’)\n",
    "for line in in_file:\n",
    "    bigline = line.rstrip(“ \\n”)\n",
    "    bigline = singular.sub(“oma”, bigline)\n",
    "    bigline = england.sub(“tumor”, bigline)\n",
    "    englishline = “”\n",
    "    hoparray = bigline.split()\n",
    "    hoparray.append(“ “)\n",
    "    for i in range(len(hoparray) - 1):\n",
    "        doublet = hoparray[i] + “ “ + hoparray[i + 1]\n",
    "        if doubhash.has_key(doublet):\n",
    "            if (englishline != “”):\n",
    "                englishline = englishline + “ “ + hoparray[i + 1]\n",
    "            else:\n",
    "                englishline = doublet\n",
    "        else:\n",
    "            if englishline != “”:\n",
    "                englishline = englishline.strip()\n",
    "                englishline = re.sub(r’^the ‘, “”, englishline)\n",
    "                englishline = re.sub(r’^in ‘, “”, englishline)\n",
    "                englishline = re.sub(r’^of ‘, “”, englishline)\n",
    "                englishline = re.sub(r’^and ‘, “”, englishline)\n",
    "                englishline = re.sub(r’^with ‘, “”, englishline)\n",
    "                englishline = re.sub(r’^from ‘, “”, englishline)\n",
    "                englishline = re.sub(r’^ a’, “”, englishline)\n",
    "                englishline = re.sub(r’ the$’, “”, englishline)\n",
    "                englishline = re.sub(r’ in$’, “”, englishline)\n",
    "                englishline = re.sub(r’ of$’, “”, englishline)\n",
    "                englishline = re.sub(r’ and$’, “”, englishline)\n",
    "                englishline = re.sub(r’ with$’, “”, englishline)\n",
    "                englishline = re.sub(r’ from$’, “”, englishline)\n",
    "                englishline = re.sub(r’ a$’, “”, englishline)\n",
    "                if literalhash.has_key(englishline):\n",
    "                    continue\n",
    "                if newhash.has_key(englishline):\n",
    "                    continue\n",
    "                phrase_match = re.search(r’ [a-z]+ ‘, englishline)\n",
    "                if phrase_match:\n",
    "                    count = count + 1\n",
    "                    print str(count) + “ “ + englishline\n",
    "                    newhash[englishline] = “”\n",
    "doubhash.close()\n",
    "literalhash.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Terms to the Neoplasm Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string\n",
    "vocab_in = open(“c:\\\\ftp\\\\neocl.xml”, “r”)\n",
    "doub_hash = {}\n",
    "for line in vocab_in:\n",
    "    code_match = re.search(r’C[0-9]{7}’, line)\n",
    "    if not code_match:\n",
    "        continue\n",
    "    line_match = re.search(r’\\”\\> ?(.+) ?\\<\\/’, line)\n",
    "    if line_match:\n",
    "        phrase = line_match.group(1)\n",
    "        doub_hash[phrase] = “”\n",
    "vocab_in.close()\n",
    "candidate_file = open(“c:\\\\ftp\\\\neocl.lst”, “r”)\n",
    "out_file = open(“new.out”, “w”)\n",
    "for line in candidate_file:\n",
    "    line = re.sub(r’\\n’,””, line)\n",
    "    if (line == “”):\n",
    "        continue\n",
    "    if doub_hash.has_key(line):\n",
    "        print line + “ already exists”\n",
    "    else:\n",
    "        print>>out_file, line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determining the Lineage of Every Neoplasm Concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.parsers.expat\n",
    "import re\n",
    "parsefile = open(‘c:\\\\ftp\\\\neocl.xml’,’r’)\n",
    "filestring = parsefile.read()\n",
    "lastname = “”\n",
    "code = “”\n",
    "count = 0\n",
    "text = “”\n",
    "def start_element(name, attrs):\n",
    "    global lastname\n",
    "    global code\n",
    "    if attrs.has_key(“nci-code”):\n",
    "        code = attrs[“nci-code”]\n",
    "    else:\n",
    "        lastname = name + “>” + lastname\n",
    "def end_element(name):\n",
    "    global count\n",
    "    global code\n",
    "    global text\n",
    "    global lastname\n",
    "    if name == “name”:\n",
    "        count = count + 1\n",
    "        print str(count) + “|” + text + “|” + code + “|” + lastname + “\\n”\n",
    "        text = “”\n",
    "    lastname = re.sub(name + r’>’,’’, lastname)\n",
    "def char_data(data):\n",
    "    global text\n",
    "    text = repr(data)\n",
    "    textmatch = re.search(r’\\’(.+)\\’’,text)\n",
    "    if textmatch:\n",
    "        text = textmatch.group(1)\n",
    "p = xml.parsers.expat.ParserCreate()\n",
    "p.StartElementHandler = start_element\n",
    "p.EndElementHandler = end_element\n",
    "p.CharacterDataHandler = char_data\n",
    "p.Parse(filestring)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
