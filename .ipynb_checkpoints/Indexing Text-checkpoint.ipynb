{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to chapter four of Methods in Medical Informatics! Book indexing consists of collecting significant words and their associated page numbers. A similar organization process can be applied to online text to improve organization and text processing speeds. We will be exploring scripts that demonstrate computational text indexing. Lets begin!\n",
    "\n",
    "> Disclaimer: The content below is adapted from the book \"Methods in Medical Informatics - Fundamental of Healthcare Programming in Perl, Python, and Ruby\" by Jules J. Berman. All content is for testing, education, and teaching purposes only. No content will be openly released to the internet. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# ZIPF Distribution of a Text File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In almost every segment of life, a small number of items usually account for the bulk of the observable activities. This pattern also hold true for words that compose a text. This phenomenon is known as Zipf's law as a mathematical description. You can write a script to illustrate the Zipf distribution for any text.*\n",
    "\n",
    "> This script will utilzied the [d2020.bin](https://datamine.unc.edu/data-files/). This is a binary file which contains tens of thousands of MeSH terms. Additional information [here](https://datamine.unc.edu/data-files/)\n",
    "\n",
    "**Description adapted from pages 53-54 of \"Methods in Medical Informatics\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T19:15:47.057612Z",
     "start_time": "2020-12-07T19:15:45.005144Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "word_list = []\n",
    "freq_list = []\n",
    "format_list = []\n",
    "freq = {}\n",
    "in_text = open('d2020.bin', \"r\", encoding=\"utf-8\")\n",
    "in_text_string = in_text.read()\n",
    "out_text = open(\"meshzipf.txt\", \"w\")\n",
    "word_list = re.findall(r'(\\b[A-Za-z][a-z]{2,15}\\b)', in_text_string)\n",
    "in_text_string = \"\"\n",
    "for item in word_list:\n",
    "    count = freq.get(item,0)\n",
    "    freq[item] = count + 1\n",
    "for key, value in freq.items():\n",
    "    value = \"000000\" + str(value)\n",
    "    value = value[-6:]\n",
    "    format_list += [value + \" \" + key]\n",
    "format_list = reversed(sorted(format_list))\n",
    "print(out_text, \"\\n\".join(format_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Script Algorithm: Zipf Distribution of a Text File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Call the necessary packages*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "word_list = []\n",
    "freq_list = []\n",
    "format_list = []\n",
    "freq = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Open the necessary file to read and create a new file, meshzipf.txt, which will receive the output of the zipf distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "in_text = open('d2020.bin', \"r\", encoding=\"utf-8\")\n",
    "in_text_string = in_text.read()\n",
    "out_text = open(\"meshzipf.txt\", \"w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Parse the string, matching against each occurrence of a latter followed by at least 2, and at most 15, lowercase letters, with the sequence bounded on either size by a word boundary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "word_list = re.findall(r'(\\b[A-Za-z][a-z]{2,15}\\b)', in_text_string)\n",
    "in_text_string = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Create a dictionary object that will include words (keys) and number of occurrences (values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for item in word_list:\n",
    "    count = freq.get(item,0)\n",
    "    freq[item] = count + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "After the dictionary object is complete, format the values in the dictionary, as a zero-padded string of uniform length. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for key, value in freq.items():\n",
    "    value = \"000000\" + str(value)\n",
    "    value = value[-6:]\n",
    "    format_list += [value + \" \" + key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Sort the key-value pairs by values, descending. Print out sorted key-value pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "format_list = reversed(sorted(format_list))\n",
    "print(out_text, \"\\n\".join(format_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**This section is adapted from section 4.1.1, \"Script Algorithm\", of page 54 from \"Methods in Medical Informatics\".*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Analysis: Zipf Distribution of a Text File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The top entries from the MeSH file are:\n",
    "\n",
    "`036645 abcdef\n",
    "034267 and\n",
    "026575 abbcdef\n",
    "017737 was\n",
    "016454 see\n",
    "014973 with\n",
    "013647 under\n",
    "010274 for\n",
    "009718 that`\n",
    "\n",
    "For these scripts, the entire content of a file is loaded into a string variable. This variable is subsequently parsed into words, with each occurrence of the word counted. If the file is very large, the script can be modified to read the file line by line, incrementing the word/frequency tally for th words contained in each line. At the top of the Zipf list are the high-frequency words, such as “the”, “and”, and “was” that serve as connectors for lower-frequency, highly specific terms. Also included at the top of the Zipf list are frequently recurring letter sequences peculiar to the file; in this case, “abcdef” and “abbcdef”. Zipf distributions have many uses in informatics projects, including the preparation of “stopword” lists.*\n",
    "\n",
    "**This section is adapted from section 4.1.2, \"Analysis\", of page 56 in \"Methods in Medical Informatics\".*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Preparing a Concordance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A concordance is a special type of index, listing every location of every word in the text. Concordances can be used to support very fast proximity searches (finding the locations of words in proximity to other words), and phrase searches (finding sequences of words located in an ordered sequence somewhere in the text. Using only a concordance, it is a simple matter to computationally recreate the entire text. Preparing a concordance is quite simple.*\n",
    "\n",
    "> This script will utilized two text files, [STOP.TXT](https://datamine.unc.edu/data-files/) and [TITLES.TXT](https://datamine.unc.edu/data-files/). STOP.TXT contains a list of stopwords. TITLES.TXT contains a list of 100 titles of journal articles. More information available [here](https://datamine.unc.edu/data-files/)\n",
    "\n",
    "**Description adapted from page 57 of \"Methods in Medical Informatics\".*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-22T01:52:26.008018Z",
     "start_time": "2020-11-22T01:52:25.876358Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "sentence_list = []\n",
    "word_list = []\n",
    "word_dict = {}\n",
    "format_list = []\n",
    "count = 0\n",
    "stopfile = open(\"./K11946_Files/STOP.TXT\",'r')\n",
    "stop_list = stopfile.readlines()\n",
    "stopfile.close()\n",
    "in_text = open('./K11946_Files/TITLES.TXT', \"r\")\n",
    "in_text_string = in_text.read()\n",
    "in_text_string = in_text_string.replace(\"\\n\",\" \")\n",
    "in_text_string = in_text_string.replace(\" +\",\" \")\n",
    "sentence_list = re.split(r'[\\.\\!\\?] +(?=[A-Z])',in_text_string)\n",
    "for sentence in sentence_list:\n",
    "    count = count + 1\n",
    "    sentence = sentence.lower()\n",
    "    word_list = re.findall(r'(\\b[a-z]{3,15}\\b)', sentence)\n",
    "    for word in word_list:\n",
    "        if word in word_dict:\n",
    "            word_dict[word] = word_dict[word] + ',' + str(count)\n",
    "        else:\n",
    "            word_dict[word] = str(count)\n",
    "keylist = word_dict.keys()\n",
    "sorted(keylist)\n",
    "for key in keylist:\n",
    "    print(key, word_dict[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Script Algorithm: Preparing a Concordance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Import the necessary packages*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "sentence_list = []\n",
    "word_list = []\n",
    "word_dict = {}\n",
    "format_list = []\n",
    "count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Read the entire contents of the titles.txt file into a string variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "stopfile = open(\"./K11946_Files/STOP.TXT\",'r')\n",
    "stop_list = stopfile.readlines()\n",
    "stopfile.close()\n",
    "in_text = open('./K11946_Files/TITLES.TXT', \"r\")\n",
    "in_text_string = in_text.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Split the file into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "in_text_string = in_text_string.replace(\"\\n\",\" \")\n",
    "in_text_string = in_text_string.replace(\" +\",\" \")\n",
    "sentence_list = re.split(r'[\\.\\!\\?] +(?=[A-Z])',in_text_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Parse each sentence into an array of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for sentence in sentence_list:\n",
    "    count = count + 1\n",
    "    sentence = sentence.lower()\n",
    "    word_list = re.findall(r'(\\b[a-z]{3,15}\\b)', sentence)\n",
    "    for word in word_list:\n",
    "        if word in word_dict:\n",
    "            word_dict[word] = word_dict[word] + ',' + str(count)\n",
    "        else:\n",
    "            word_dict[word] = str(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Add the location of the word to the dictionary object that contains the encountered words and their locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "keylist = word_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Order the words alphabetically and print out each word in the dictionary object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sorted(keylist)\n",
    "for key in keylist:\n",
    "    print(key, word_dict[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**This section is adapted from section 4.2.1, \"Script Algorithm\", of page 57 from \"Methods in Medical Informatics\".*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Analysis: Preparing a Concordance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The sample text consisted of 100 parsed sentences. Here are the first few lines of the output.*\n",
    "\n",
    "`carcinoid 1\n",
    "tumor 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100\n",
    "the 1,6,7,8,11,12,14,16,17,19,22,23,35,39,41,41,44,47,49,51,55,58,59,65,70,71,72,76,78,78,90,95,96,98\n",
    "common 1\n",
    "bile 1\n",
    "duct 1,66\n",
    "rare 1,39\n",
    "complication 1`\n",
    "\n",
    "**This section is adapted from section 4.2.2, \"Analysis\", of pages 59-60 from \"Methods in Medical Informatics\".*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Extracting Phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "All text is composed of words and phrases that represent specific concepts, that are connected together into a sequence of meaningful statements. One way to extract useful concepts is to remove common words or \"stopwords\". This script will demonstrate phrase extraction through stopword removal.*\n",
    "\n",
    "> This script will utilized the text files [STOP.TXT](https://datamine.unc.edu/data-files/) and [cancer_gene_titles.txt](https://datamine.unc.edu/data-files/). STOP.TXT contains a list of common stopwords. cancer_gene_titles.txt contains a list of cancer-related journal articles extracted from a PubMed query. More information [here](https://datamine.unc.edu/data-files/)\n",
    "\n",
    "**Description adapted from page 60 of \"Methods in Medical Informatics\".*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-22T02:02:42.477637Z",
     "start_time": "2020-11-22T02:01:57.226458Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import re, string\n",
    "item_list = []\n",
    "stopfile = open(\"./K11946_Files/STOP.TXT\",'r')\n",
    "stop_list = stopfile.readlines()\n",
    "stopfile.close()\n",
    "in_text = open(\"./K11946_Files/cancer_gene_titles.txt\", \"r\")\n",
    "count = 0\n",
    "for line in in_text:\n",
    "    count = count + 1\n",
    "    for stopword in stop_list:\n",
    "        stopword = re.sub(r'\\n', '', stopword)\n",
    "        line = re.sub(r' *\\b' + stopword + r'\\b *', '\\n', line)\n",
    "    item_list.extend(line.split(\"\\n\"))\n",
    "item_list = sorted(set(item_list))\n",
    "out_text = open('phrases.txt', \"w\")\n",
    "for item in item_list:\n",
    "    print(out_text, item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Script Algorithm: Extracting Phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Call necessary packages*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import re, string\n",
    "item_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Open the stop.txt file, containing a list of common stopwords. Split into a list structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "stopfile = open(\"./K11946_Files/STOP.TXT\",'r')\n",
    "stop_list = stopfile.readlines()\n",
    "stopfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Open cancer_gene_titles.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "in_text = open(\"./K11946_Files/cancer_gene_titles.txt\", \"r\")\n",
    "count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Pare through the lines of the text. Substittue a newline character for every occurrence of any stopword in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for line in in_text:\n",
    "    count = count + 1\n",
    "    for stopword in stop_list:\n",
    "        stopword = re.sub(r'\\n', '', stopword)\n",
    "        line = re.sub(r' *\\b' + stopword + r'\\b *', '\\n', line)\n",
    "    item_list.extend(line.split(\"\\n\"))\n",
    "item_list = sorted(set(item_list))\n",
    "out_text = open('phrases.txt', \"w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Sort item alphabetically and print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for item in item_list:\n",
    "    print(out_text, item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**This section is adapted from section 4.3.1, \"Script Algorithm\", of page 61 from \"Methods in Medical Informatics\".*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Analysis: Extracting Phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The output is an alphabetic file of the phrases that might appear in a book's index. We used the file consisting of titles from a PubMed search. This file, cancer_gene_titles.txt, is about 1.1 MB in length, the size of a typical book. We only required about a dozen lines of code and a few seconds of execution time to create out list of index terms.*\n",
    "\n",
    "**This section is adapted from section 4.3.2, \"Analysis\", of page 63 from \"Methods in Medical Informatics\".*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Preparing an Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "An index is a list of the important words or phrases contained in a book, along with the locations where each of those words and phrases can be found. This is different from concordance because the index does not contain every word found in the text, and the index contains groups of selected phrases, in addition to individual words. Software can be used to create indexes. However, remember that a useful index is more selective than simply recording the location of every word and phrase.*\n",
    "\n",
    "> This script will utilized the text files [STOP.TXT](https://datamine.unc.edu/data-files/) and [TEXT.txt](https://datamine.unc.edu/data-files/). STOP.TXT contains a list of common stopwords. TEXT.txt contains a sample journal article. More information [here](https://datamine.unc.edu/data-files/)\n",
    "\n",
    "\n",
    "**Description adapted from page 63-64 of \"Methods in Medical Informatics\".*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T18:33:14.795737Z",
     "start_time": "2020-12-06T18:33:14.597778Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "item_list = []\n",
    "item_dictionary = {}\n",
    "place_string = \"\"\n",
    "stopfile = open(\"./K11946_Files/STOP.TXT\",'r')\n",
    "stop_list = stopfile.readlines()\n",
    "stopfile.close()\n",
    "in_text = open('./K11946_Files/TEXT.TXT', 'r')\n",
    "in_text_string = in_text.read()\n",
    "in_text_string = in_text_string.replace(\"\\n\",\" \")\n",
    "in_text_string = in_text_string.replace(\" +\",\" \")\n",
    "sentence_list = re.split(r'[\\.\\!\\?] +(?=[A-Z])',in_text_string)\n",
    "norm = str.maketrans('','',string.printable)\n",
    "badascii = str()\n",
    "badascii = badascii.translate(norm)\n",
    "badascii_table = badascii + (256 - len(badascii))*\" \"\n",
    "junk_table = 256*\" \"\n",
    "table = str.maketrans(badascii_table,junk_table)\n",
    "count = 0\n",
    "for item in sentence_list:\n",
    "    count = count + 1\n",
    "    count_string = str(count)\n",
    "    item = item.lower()\n",
    "    item = re.sub(r'\\'s', \"\", item)\n",
    "    item = item.translate(table)\n",
    "    for stopword in stop_list:\n",
    "        stopword = stopword.rstrip()\n",
    "        item = re.sub(r' *\\b' + stopword + r'\\b *', '\\n', item)\n",
    "    item_list = item.split(\"\\n\")\n",
    "    for phrase in item_list:\n",
    "        phrasematch = re.match(r'^[0-9]', phrase)\n",
    "        if (phrasematch):\n",
    "            continue\n",
    "        if phrase in item_dictionary:\n",
    "            item_dictionary[phrase] = item_dictionary[phrase] + ',' + count_string\n",
    "        else:\n",
    "            item_dictionary[phrase] = count_string\n",
    "keylist = item_dictionary.keys()\n",
    "keylist = sorted(keylist)\n",
    "for key in keylist:\n",
    "    print(key, item_dictionary[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Script Algorithm: Preparing an Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Create an array containing stopwords. You can use any stopword list you prefer.\n",
    "In this script, we use stop.txt available at http://datamine.unc.edu/jupyter/edit/Methods-in-Medical-Informatics-master/K11946_Files/STOP.TXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "item_list = []\n",
    "item_dictionary = {}\n",
    "place_string = \"\"\n",
    "stopfile = open(\"./K11946_Files/STOP.TXT\",'r')\n",
    "stop_list = stopfile.readlines()\n",
    "stopfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Open a file to be indexed. You can use any file, but in this text, we use text.\n",
    "txt, available at http://www.julesberman.info/book/text.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "in_text = open('./K11946_Files/TEXT.TXT', 'r')\n",
    "in_text_string = in_text.read()\n",
    "in_text_string = in_text_string.replace(\"\\n\",\" \")\n",
    "in_text_string = in_text_string.replace(\" +\",\" \")\n",
    "sentence_list = re.split(r'[\\.\\!\\?] +(?=[A-Z])',in_text_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Strip the text of any non-ASCII characters (not necessary if you are using a\n",
    "plain-text file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "in_text_string = in_text.read()\n",
    "in_text_string = in_text_string.replace(\"\\n\",\" \")\n",
    "in_text_string = in_text_string.replace(\" +\",\" \")\n",
    "sentence_list = re.split(r'[\\.\\!\\?] +(?=[A-Z])',in_text_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Split the text into sentences and put the consecutive sentences into an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sentence_list = re.split(r'[\\.\\!\\?] +(?=[A-Z])',in_text_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Create a dictionary object, which will hold phrases as keys and a commaseparated\n",
    "list of numbers, representing the sentences in which the phrases\n",
    "appear, as the values. For each sentence in the array of consecutive sentences, split the sentence\n",
    "wherever a stopword appears, and put the resulting phrases into an array. For each array of phrases, from each sentence, parse through the array of\n",
    "phrases, assigning each phrase to a dictionary key, and concatenating the sentence\n",
    "number in which the phrase occurs, to the comma-separated list of sentence\n",
    "numbers that serves as the value for the key (phrase)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "norm = str.maketrans('','',string.printable)\n",
    "badascii = str()\n",
    "badascii = badascii.translate(norm)\n",
    "badascii_table = badascii + (256 - len(badascii))*\" \"\n",
    "junk_table = 256*\" \"\n",
    "table = str.maketrans(badascii_table,junk_table)\n",
    "count = 0\n",
    "for item in sentence_list:\n",
    "    count = count + 1\n",
    "    count_string = str(count)\n",
    "    item = item.lower()\n",
    "    item = re.sub(r'\\'s', \"\", item)\n",
    "    item = item.translate(table)\n",
    "    for stopword in stop_list:\n",
    "        stopword = stopword.rstrip()\n",
    "        item = re.sub(r' *\\b' + stopword + r'\\b *', '\\n', item)\n",
    "    item_list = item.split(\"\\n\")\n",
    "    for phrase in item_list:\n",
    "        phrasematch = re.match(r'^[0-9]', phrase)\n",
    "        if (phrasematch):\n",
    "            continue\n",
    "        if phrase in item_dictionary:\n",
    "            item_dictionary[phrase] = item_dictionary[phrase] + ',' + count_string\n",
    "        else:\n",
    "            item_dictionary[phrase] = count_string\n",
    "keylist = item_dictionary.keys()\n",
    "keylist = sorted(keylist)\n",
    "for key in keylist:\n",
    "    print(key, item_dictionary[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**This section is adapted from section 4.4.1, \"Script Algorithm\", of page 65 from \"Methods in Medical Informatics\".*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Analysis: Preparing an Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "An example of the kind of output produced by the script is shown\n",
    "\n",
    "`adjustment 7,9\n",
    "adjuvant chemotherapy 83\n",
    "adjuvant imrt 23\n",
    "`\n",
    "\n",
    "The numbers represent the sentence numbers in which each phrase occurs. AUtomated indexing invariably produces a product that a human indexer can improve. The strength of automatic indexing is found when the texts are very long. Humans cannot index long texts. A flawed computer-generated index is usually better than no index at all*\n",
    "\n",
    "**This section is adapted from section 4.4.2, \"Analysis\", of page 68 from \"Methods in Medical Informatics\".*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Comparing Texts Using Similarity Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "When you have extracted all of the phrases occurring in a text, you have created something akin to the signature of the text. We can then determine whether two different text are similar, when we compare their signatures. Similarity scores are very useful in medical science. We can use similarity scores to establish relatedness of objects (ie. DNA sequences), to find trends and outliers in population data, to provide \"best-fit\" search results, and to classify groups of items. This script will demonstrate calculating the similarity between two documents using Pearson correlation.*\n",
    "\n",
    "> This script will utilized the text files [STOP.TXT](https://datamine.unc.edu/data-files/), [paradise.txt](https://datamine.unc.edu/data-files/), and [treasure.txt](https://datamine.unc.edu/data-files/). STOP.TXT contains a list of common stopwords. paradise.txt contains the novel **Paradise Lost** in text format. treasure.txt contains the novel **Treasure Island** in text format. More information [here](https://datamine.unc.edu/data-files/)\n",
    "\n",
    "\n",
    "**This section is adapted from page 69 of \"Methods in Medical Informatics\".*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T18:38:53.093437Z",
     "start_time": "2020-12-06T18:38:41.214354Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from math import sqrt\n",
    "from math import pow\n",
    "treasure = {}\n",
    "paradise = {}\n",
    "filelist = [\"./K11946_Files/treasure.txt\", \"./K11946_Files/paradise.txt\"]\n",
    "stopfile = open(\"./K11946_Files/stop.txt\",'r')\n",
    "stop_list = stopfile.readlines()\n",
    "stopfile.close()\n",
    "phraseform = re.compile(r'^[a-z]+ [a-z ]+$')\n",
    "for filename in filelist:\n",
    "    in_text = open(filename, \"r\", encoding=\"utf-8\")\n",
    "    in_text_string = in_text.read()\n",
    "    in_text.close()\n",
    "    in_text_string = in_text_string.replace(\"\\n\",\" \")\n",
    "    for stopword in stop_list:\n",
    "        stopword = stopword.rstrip()\n",
    "        in_text_string = re.sub(r' *\\b' + stopword + r'\\b *', '\\n',in_text_string)\n",
    "    in_text_string = re.sub(r'[\\,\\:\\;\\(\\)]','\\n',in_text_string)\n",
    "    in_text_string = re.sub(r'[\\.\\!\\?] +(?=[A-Z])', '\\n', in_text_string)\n",
    "    in_text_string = in_text_string.lower()\n",
    "    item_list = re.split(r' *\\n *', in_text_string)\n",
    "    for phrase in item_list:\n",
    "        phrase = re.sub(r' +',' ', phrase)\n",
    "        phrase = phrase.strip()\n",
    "        phrasematch = phraseform.match(phrase)\n",
    "        if not (phrasematch):\n",
    "            continue\n",
    "        if (filename == \"./K11946_Files/paradise.txt\"):\n",
    "            if phrase in paradise:\n",
    "                paradise[phrase] = paradise[phrase] + 1\n",
    "            else:\n",
    "                paradise[phrase] = 1\n",
    "            if not (phrase in treasure):\n",
    "                treasure[phrase] = 0\n",
    "        if (filename == \"./K11946_Files/treasure.txt\"):\n",
    "            if phrase in treasure:\n",
    "                treasure[phrase] = treasure[phrase] + 1\n",
    "            else:\n",
    "                treasure[phrase] = 1\n",
    "            if not (phrase in paradise):\n",
    "                paradise[phrase] = 0\n",
    "count = 0; sumtally1 = 0; sumtally2 = 0; sqtally1 = 0; sqtally2 = 0\n",
    "prodtally12 = 0; part1 = 0; part2 = 0; part3 = 0;\n",
    "keylist = paradise.keys()\n",
    "for key in keylist:\n",
    "    count = count + 1;\n",
    "    sumtally1 = sumtally1 + paradise[key]\n",
    "    sumtally2 = sumtally2 + treasure[key]\n",
    "    sqtally1 = sqtally1 + pow(paradise[key],2)\n",
    "    sqtally2 = sqtally2 + pow(treasure[key],2)\n",
    "    prodtally12 = prodtally12 + (paradise[key] * treasure[key])\n",
    "part1 = prodtally12 - (float(sumtally1 * sumtally2) / count)\n",
    "part2 = sqtally1 - (float(pow(sumtally1,2)) / count)\n",
    "part3 = sqtally2 - (float(pow(sumtally2,2)) / count)\n",
    "similarity12 = float(part1) / float(sqrt(part2 * part3))\n",
    "print(\"The Pearson score is\", similarity12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Script Algorithm: Comparing Texts Using Similarity Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We could compare any two documents, but for this exercise we chose\n",
    "Stevenson’s Treasure Island and Milton’s Paradise Lost. The two novels represent\n",
    "very different writing styles. The etext versions of these books are publicly available and can be downloaded from Project Gutenberg at the following\n",
    "URLs:\n",
    "<br>\n",
    "<br>\n",
    "For Paradise Lost:\n",
    "<br>\n",
    "http://www.gutenberg.org/dirs/etext91/plboss10.txt\n",
    "<br>\n",
    "For Treasure Island:\n",
    "<br>\n",
    "http://www.gutenberg.org/etext/120\n",
    "\n",
    "Put the names of each text file into an array. We will be performing the same\n",
    "parsing steps on each of the two files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from math import sqrt\n",
    "from math import pow\n",
    "treasure = {}\n",
    "paradise = {}\n",
    "filelist = [\"./K11946_Files/treasure.txt\", \"./K11946_Files/paradise.txt\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Open the stop.txt file, containing the high-frequency stopwords that we will\n",
    "use to determine the boundaries of a phrase. (Remember: An index phrase is\n",
    "a sequence of words bounded on both sides by a stop word or by the beginning\n",
    "or the end of a sentence.) The stop file consists of one word per file line.\n",
    "Put all of the words from the stop.txt file into an array, stripping the newline\n",
    "character that separates each stop word from the subsequent stop word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "stopfile = open(\"./K11946_Files/stop.txt\",'r')\n",
    "stop_list = stopfile.readlines()\n",
    "stopfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Open the first text file (Paradise Lost), and read the entire text into a\n",
    "string variable. Delete every newline character from the text file string, replacing it with a\n",
    "space character. In the text file string, wherever there is a sequence of words bounded on either\n",
    "side by a stopword, replace the stopwords with a newline character. Iterate\n",
    "this determination and replacement, over the entire text file string, for every\n",
    "stopword in our array of stop words. Wherever there is a “,”, “:”, “;”, “(“ or ”)” in the text file string, replace the punctuation\n",
    "with a newline character. We do this because these punctuation marks\n",
    "delineate the beginning and the end of an expression and, for the purposes of\n",
    "delineating index phrases, these punctuation marks are equivalent to an endof-\n",
    "sentence marker. Wherever the text file string has a “.”, or “!” or “?” followed by one or more spaces,\n",
    "followed by an uppercase letter, replace the punctuation and the following white\n",
    "spaces with a newline character. We do this because the pattern is typical of a\n",
    "sentence ending, and sentence endings mark the end of index phrases. Convert the modified text file string, which now marks the beginning and\n",
    "ending of index phrases with newline characters, into lowercase.\n",
    "Convert the modified text file string, replacing all sequences consisting of\n",
    "multiple space characters with a single space character.\n",
    "Split the text file string into an array, at every occurrence of a newline character\n",
    "bordered by zero or more spaces. This results in an array that includes all\n",
    "of the index phrases in the original text file.\n",
    "Iterate through every phrase in the newly created array of index phrases.\n",
    "For each phrase, if the phrase does not match a sequence of lowercase letters\n",
    "followed by a space followed by a sequence of lowercase letters or spaces, skip\n",
    "to the next item in the phrase array. We do this primarily to eliminate single\n",
    "word phrases that do not contain a space intervening between words. This step\n",
    "also eliminates phrases that contain numeric and nonalphabet characters.\n",
    "We will be using two dictionary objects: the dictionary object consisting of all\n",
    "of the index phrases from Paradise Lost as keys, and the number of occurrences\n",
    "of each index phrase in Paradise Lost as the values, as well as the index phrases\n",
    "that occur exclusively in Treasure Island, all with the number “0” as the value.\n",
    "The other dictionary object will consist of the index phrases from Treasure\n",
    "Island as keys, and the number of occurrences of each index phrase from\n",
    "Treasure Island, as the values, as well as the index phrases that occur exclusively\n",
    "in Paradise Lost, all with the number “0” as the value. By creating these two\n",
    "dictionary objects, we create two dictionary objects that have the same matching\n",
    "set of keys, with one set of keys holding the number of occurrences of the\n",
    "keys in Paradise Lost, and the other holding the number of occurrences of the\n",
    "keys in Treasure Island. We can then compare each dictionary object key by\n",
    "key and value by value. To create the two dictionary objects, increment each occurrence of a phrase\n",
    "by one in the dictionary object for the text file in which it has occurred, and\n",
    "create a key–value pair in the other text file’s dictionary object (if none exists)\n",
    "consisting of the phrase and the value “0”. Repeat steps 4 to 15 for the second book, Treasure Island. When you have\n",
    "repeated these steps for the second book you will have collected the two\n",
    "dictionary objects that you will use to compute the Pearson score. At this\n",
    "point, you could substitute any similarity correlation scores you prefer over the\n",
    "Pearson score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "phraseform = re.compile(r'^[a-z]+ [a-z ]+$')\n",
    "for filename in filelist:\n",
    "    in_text = open(filename, \"r\", encoding=\"utf-8\")\n",
    "    in_text_string = in_text.read()\n",
    "    in_text.close()\n",
    "    in_text_string = in_text_string.replace(\"\\n\",\" \")\n",
    "    for stopword in stop_list:\n",
    "        stopword = stopword.rstrip()\n",
    "        in_text_string = re.sub(r' *\\b' + stopword + r'\\b *', '\\n',in_text_string)\n",
    "    in_text_string = re.sub(r'[\\,\\:\\;\\(\\)]','\\n',in_text_string)\n",
    "    in_text_string = re.sub(r'[\\.\\!\\?] +(?=[A-Z])', '\\n', in_text_string)\n",
    "    in_text_string = in_text_string.lower()\n",
    "    item_list = re.split(r' *\\n *', in_text_string)\n",
    "    for phrase in item_list:\n",
    "        phrase = re.sub(r' +',' ', phrase)\n",
    "        phrase = phrase.strip()\n",
    "        phrasematch = phraseform.match(phrase)\n",
    "        if not (phrasematch):\n",
    "            continue\n",
    "        if (filename == \"./K11946_Files/paradise.txt\"):\n",
    "            if phrase in paradise:\n",
    "                paradise[phrase] = paradise[phrase] + 1\n",
    "            else:\n",
    "                paradise[phrase] = 1\n",
    "            if not (phrase in treasure):\n",
    "                treasure[phrase] = 0\n",
    "        if (filename == \"./K11946_Files/treasure.txt\"):\n",
    "            if phrase in treasure:\n",
    "                treasure[phrase] = treasure[phrase] + 1\n",
    "            else:\n",
    "                treasure[phrase] = 1\n",
    "            if not (phrase in paradise):\n",
    "                paradise[phrase] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Parse over every key–value pair in either dictionary object (we chose the dictionary\n",
    "object for Paradise Lost, but the calculation, which depends on differences\n",
    "between the two dictionary objects, would yield the same score using\n",
    "either dictionary object). Keep a count of the total number of key–value pairs. Produce a summation tally of the values in the Paradise Lost dictionary object\n",
    "and in the Treasure Island dictionary object. Produce a summation tally of the squares of the values in the Paradise Lost dictionary\n",
    "object and the squares of the values in the Treasure Island dictionary object. Produce a summation tally of the products of each value in the Paradise Lost\n",
    "dictionary object multiplied by the corresponding value (the value of the same\n",
    "key) in the Treasure Island dictionary object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "count = 0; sumtally1 = 0; sumtally2 = 0; sqtally1 = 0; sqtally2 = 0\n",
    "prodtally12 = 0; part1 = 0; part2 = 0; part3 = 0;\n",
    "keylist = paradise.keys()\n",
    "for key in keylist:\n",
    "    count = count + 1;\n",
    "    sumtally1 = sumtally1 + paradise[key]\n",
    "    sumtally2 = sumtally2 + treasure[key]\n",
    "    sqtally1 = sqtally1 + pow(paradise[key],2)\n",
    "    sqtally2 = sqtally2 + pow(treasure[key],2)\n",
    "    prodtally12 = prodtally12 + (paradise[key] * treasure[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "After the dictionary object is parsed, you will take the tally variables that you\n",
    "just computed, and you will insert them into the Pearson formula.\n",
    "The Pearson score is the summation tally of the products minus the sum tally\n",
    "of the first dictionary object times the sum tally of the second dictionary object\n",
    "divided by the number of keys in the object all divided by the square root of\n",
    "the tally of the squares of the values of the Paradise Lost dictionary object\n",
    "times the square of the sum tally of Paradise Lost dictionary object divided\n",
    "by the number of keys in the object, times the tally of the squares of the values\n",
    "of the Treasure Island dictionary object times the square of the sum tally of\n",
    "Treasure Island dictionary object divided by the number of keys in the object.\n",
    "Step 23 is an example where the description of a mathematical expression, in\n",
    "English, is much, much more confusing than the program code for the mathematical\n",
    "expression.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "part1 = prodtally12 - (float(sumtally1 * sumtally2) / count)\n",
    "part2 = sqtally1 - (float(pow(sumtally1,2)) / count)\n",
    "part3 = sqtally2 - (float(pow(sumtally2,2)) / count)\n",
    "similarity12 = float(part1) / float(sqrt(part2 * part3))\n",
    "print(\"The Pearson score is\", similarity12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T19:17:44.548355Z",
     "start_time": "2020-12-06T19:17:44.536877Z"
    },
    "hidden": true
   },
   "source": [
    "**This section is adapted from section 4.5.1, \"Script Algorithm\", of pages 69-70 from \"Methods in Medical Informatics\".*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Analysis: Comparing Texts Using Similarity Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Pearson scores range from -1 to 1. A score of 1 occurs when a document is compared against itself. When we compute the Pearson score between two highly dissimilar texts, the yielded score is -0.38257. We expected and received a low-end Pearson score.*\n",
    "\n",
    "**This section is adapted from section 4.5.2, \"Analysis\", of page 76 from \"Methods in Medical Informatics\".*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
